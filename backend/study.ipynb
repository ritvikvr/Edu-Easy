{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u8k26WKLgyX",
        "outputId": "bfd9e73a-7b17-43a0-9015-31bc78d15a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.12/dist-packages (1.17.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.182.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.9)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y poppler-utils\n",
        "!pip install pytesseract pdf2image Pillow faiss-cpu sentence-transformers google-generativeai langchain\n",
        "\n",
        "import os\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pickle\n",
        "import faiss\n",
        "import logging\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_images(pdf_path):\n",
        "    return convert_from_path(pdf_path, dpi=300)\n",
        "\n",
        "def ocr_image(image: Image.Image):\n",
        "    return pytesseract.image_to_string(image)\n",
        "\n",
        "def ocr_pdf(pdf_path):\n",
        "    images = pdf_to_images(pdf_path)\n",
        "    return [ocr_image(img) for img in images]\n",
        "\n",
        "def chunk_text(text, chunk_size=500, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def chunk_pdf_texts(texts, filename):\n",
        "    all_chunks = []\n",
        "    for page_no, text in enumerate(texts, 1):\n",
        "        for chunk in chunk_text(text):\n",
        "            all_chunks.append({\n",
        "                \"text\": chunk,\n",
        "                \"metadata\": {\"filename\": filename, \"page_no\": page_no}\n",
        "            })\n",
        "    return all_chunks"
      ],
      "metadata": {
        "id": "b3UzvJQINik5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def get_local_embedding(text):\n",
        "    return model.encode(text)\n",
        "\n",
        "def build_faiss_index(chunks):\n",
        "    embeddings = [get_local_embedding(chunk['text']) for chunk in chunks]\n",
        "    embeddings = np.array(embeddings).astype('float32')\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embeddings)\n",
        "    return index, embeddings\n",
        "\n",
        "def save_index(index, chunks, path=\"faiss_index\"):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    faiss.write_index(index, f\"{path}/index.faiss\")\n",
        "    with open(f\"{path}/metadata.pkl\", \"wb\") as f:\n",
        "        pickle.dump(chunks, f)\n"
      ],
      "metadata": {
        "id": "S0blsxy4Nner"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_faiss(query, index, chunks, k=5):\n",
        "    query_vector = model.encode([query])\n",
        "    faiss.normalize_L2(query_vector)\n",
        "    D, I = index.search(query_vector.astype('float32'), k)\n",
        "    return [chunks[i] for i in I[0] if i != -1]\n",
        "\n",
        "def search_bm25(query, chunks, k=5):\n",
        "    return []\n"
      ],
      "metadata": {
        "id": "omoY1jYtNpdx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(\".env\", \"w\") as f:\n",
        "    f.write(\"GOOGLE_API_KEY=AIzaSyCoa772_NIMHiGY8mW5jNTwiN6wSGI3-ck\")\n"
      ],
      "metadata": {
        "id": "uMs3OXSlNrVD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    raise ValueError(\"API key not found\")\n",
        "\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key=api_key)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRkc_QoJNt1N",
        "outputId": "11dce547-061f-40de-d085-e59834442db6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "class ChatService:\n",
        "    def __init__(self):\n",
        "\n",
        "        load_dotenv()\n",
        "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "        if not api_key:\n",
        "            raise ValueError(\"GOOGLE_API_KEY not found in .env file.\")\n",
        "\n",
        "\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "\n",
        "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "        self.chat = self.model.start_chat(history=[])\n",
        "\n",
        "    def ask(self, prompt):\n",
        "        response = self.chat.send_message(prompt)\n",
        "        return response.text\n",
        "\n",
        "\n",
        "class LocalRetriever:\n",
        "    def __init__(self, index, chunks):\n",
        "        self.index = index\n",
        "        self.chunks = chunks\n",
        "\n",
        "    def get_relevant_documents(self, query):\n",
        "        results = search_faiss(query, self.index, self.chunks)\n",
        "        if not results:\n",
        "            results = search_bm25(query, self.chunks)\n",
        "        return [Document(page_content=chunk['text'], metadata=chunk['metadata']) for chunk in results]\n"
      ],
      "metadata": {
        "id": "9T7fxqMFNvjV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "def pdf_to_images(pdf_path):\n",
        "    return convert_from_path(pdf_path, dpi=300)\n",
        "\n",
        "def ocr_image(image: Image.Image):\n",
        "    return pytesseract.image_to_string(image)\n",
        "\n",
        "def ocr_pdf(pdf_path):\n",
        "    images = pdf_to_images(pdf_path)\n",
        "    return [ocr_image(img) for img in images]\n",
        "\n",
        "def chunk_text(text, chunk_size=500, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def chunk_pdf_texts(texts, filename):\n",
        "    all_chunks = []\n",
        "    for page_no, text in enumerate(texts, 1):\n",
        "        for chunk in chunk_text(text):\n",
        "            all_chunks.append({\n",
        "                \"text\": chunk,\n",
        "                \"metadata\": {\"filename\": filename, \"page_no\": page_no}\n",
        "            })\n",
        "    return all_chunks\n",
        "\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "pdf_path = \"/content/DBMS Unit 5.pdf\"\n",
        "ocr_results = ocr_pdf(pdf_path)\n",
        "filename = os.path.basename(pdf_path)\n",
        "chunks = chunk_pdf_texts(ocr_results, filename)\n",
        "\n",
        "\n",
        "index, _ = build_faiss_index(chunks)\n",
        "save_index(index, chunks)\n",
        "\n",
        "retriever = LocalRetriever(index, chunks)\n",
        "gemini_chat = ChatService()\n",
        "\n",
        "\n",
        "query = \"What are CRUD Operations?\"\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "\n",
        "final_response = gemini_chat.ask(f\"Given the following context, answer this: {query}\\n\\n{context}\")\n",
        "\n",
        "\n",
        "print(\"Answer:\", final_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "w7s6ifb2cQ87",
        "outputId": "43e77d33-36e2-45b5-b9a4-47fc2d7b81aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Based on the provided text, CRUD operations are the fundamental operations performed on data in a database.  The acronym CRUD stands for:\n",
            "\n",
            "* **C**reate\n",
            "* **R**ead\n",
            "* **U**pdate\n",
            "* **D**elete\n",
            "\n",
            "The text specifically mentions CRUD operations in the context of MongoDB and HBase, which are NoSQL databases.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# EXTRA FUNCTIONS ADDED\n",
        "# ------------------------------\n",
        "\n",
        "def summarize_pdf(chunks, gemini_chat, max_pages=5):\n",
        "    \"\"\"\n",
        "    Summarize the PDF by giving Gemini a chunked context.\n",
        "    You can limit pages using `max_pages` to control size.\n",
        "    \"\"\"\n",
        "    summaries = []\n",
        "    for i, chunk in enumerate(chunks[:max_pages]):\n",
        "        context = chunk[\"text\"]\n",
        "        prompt = f\"Summarize the following text from page {chunk['metadata']['page_no']}:\\n\\n{context}\"\n",
        "        response = gemini_chat.ask(prompt)\n",
        "        summaries.append(f\"Page {chunk['metadata']['page_no']} Summary:\\n{response}\\n\")\n",
        "    return \"\\n\".join(summaries)\n",
        "\n",
        "\n",
        "def generate_mcqs(chunks, gemini_chat, num_mcqs=5):\n",
        "    \"\"\"\n",
        "    Generate MCQs based on the PDF chunks.\n",
        "    \"\"\"\n",
        "    # Take all text as one big context\n",
        "    context = \"\\n\".join([chunk[\"text\"] for chunk in chunks[:10]])  # first 10 chunks for efficiency\n",
        "    prompt = f\"\"\"\n",
        "    Based on the following study material, generate {num_mcqs} multiple-choice questions (MCQs).\n",
        "    Each MCQ should have 4 options (A, B, C, D) and specify the correct answer clearly.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "    \"\"\"\n",
        "    response = gemini_chat.ask(prompt)\n",
        "    return response\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# USAGE\n",
        "# ------------------------------\n",
        "\n",
        "# Summarization\n",
        "print(\"\\n--- PDF SUMMARY ---\\n\")\n",
        "pdf_summary = summarize_pdf(chunks, gemini_chat)\n",
        "print(pdf_summary)\n",
        "\n",
        "# MCQ Generation\n",
        "print(\"\\n--- GENERATED MCQs ---\\n\")\n",
        "mcqs = generate_mcqs(chunks, gemini_chat, num_mcqs=5)\n",
        "print(mcqs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rd-PuBWGdZR8",
        "outputId": "34cd28cd-1e5f-476e-c5ec-b523a5faa599"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- PDF SUMMARY ---\n",
            "\n",
            "Page 1 Summary:\n",
            "The syllabus covers database concepts, starting with conceptual data modeling (including ER and EER models and UML diagrams), relational database design (including SQL, normalization up to BCNF, and handling update anomalies), transaction management (ACID properties, concurrency control, and recovery), and finally, an introduction to NoSQL databases (MongoDB and HBase, focusing on their data models and CRUD operations).\n",
            "\n",
            "\n",
            "Page 2 Summary:\n",
            "Unit V focuses on NoSQL databases, covering the CAP theorem and the data models and CRUD operations of document-based databases (like MongoDB) and column-based databases (like HBase).\n",
            "\n",
            "\n",
            "Page 3 Summary:\n",
            "This snippet only shows \"Column-Family Analytical\" and the university name.  There's no substantive content to summarize.  It likely refers to a characteristic or topic related to column-oriented databases discussed earlier in the omitted text.\n",
            "\n",
            "\n",
            "Page 4 Summary:\n",
            "NoSQL databases utilize diverse data models to handle large volumes of data with low latency.  The text mentions document databases and wide-column databases as examples of these NoSQL database types.\n",
            "\n",
            "\n",
            "Page 5 Summary:\n",
            "A distributed database stores and runs across multiple computers; NoSQL databases are an example of this type of database.\n",
            "\n",
            "\n",
            "\n",
            "--- GENERATED MCQs ---\n",
            "\n",
            "Here are 5 multiple-choice questions based on the provided text:\n",
            "\n",
            "**1. Which of the following is NOT a stage typically included in the Database System Development Lifecycle?**\n",
            "A) Requirements Collection\n",
            "B) Database Design\n",
            "C) Software Testing\n",
            "D) Conceptual Data Modeling\n",
            "\n",
            "**Correct Answer: C**\n",
            "\n",
            "\n",
            "**2.  In the context of relational database design, what does BCNF refer to?**\n",
            "A) Basic Canonical Form\n",
            "B) Boyce-Codd Normal Form\n",
            "C) Best Case Normalization Form\n",
            "D) Binary-Coded Normalization Form\n",
            "\n",
            "**Correct Answer: B**\n",
            "\n",
            "\n",
            "**3. The ACID properties are related to which database concept?**\n",
            "A) NoSQL Databases\n",
            "B) Data Modeling\n",
            "C) Transactions\n",
            "D) Normalization\n",
            "\n",
            "**Correct Answer: C**\n",
            "\n",
            "\n",
            "**4.  According to the CAP theorem, a database system can only fully satisfy which combination of properties?**\n",
            "A) Consistency, Availability, and Partition Tolerance\n",
            "B) Consistency and Availability\n",
            "C) Availability and Partition Tolerance or Consistency and Partition Tolerance\n",
            "D) Consistency and Partition Tolerance\n",
            "\n",
            "**Correct Answer: C**  (Note: The CAP theorem states that you can only pick two out of three.  The phrasing here attempts to reflect that.)\n",
            "\n",
            "\n",
            "**5.  Which type of NoSQL database is described as consisting of sets of key-value pairs stored in a document?**\n",
            "A) Wide-Column Database\n",
            "B) Graph Database\n",
            "C) Key-Value Store\n",
            "D) Document Database\n",
            "\n",
            "**Correct Answer: D**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "# ------------------------------\n",
        "# JSON WRAPPER FUNCTION\n",
        "# ------------------------------\n",
        "def create_json_output(pdf_summary, user_query, final_answer, mcqs):\n",
        "    \"\"\"\n",
        "    Wraps summarizer, user query & answer, and MCQs into a JSON object.\n",
        "    \"\"\"\n",
        "    output = {\n",
        "        \"summariser\": pdf_summary,\n",
        "        \"user_query\": {\n",
        "            \"query\": user_query,\n",
        "            \"answer\": final_answer\n",
        "        },\n",
        "        \"mcqs\": mcqs\n",
        "    }\n",
        "    return json.dumps(output, indent=4)\n",
        "\n",
        "\n",
        "def save_json_to_file(json_data, filename=\"results.json\"):\n",
        "    \"\"\"\n",
        "    Saves JSON string to a file and triggers download in Colab.\n",
        "    \"\"\"\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(json_data)\n",
        "    files.download(filename)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# EXISTING PIPELINE FUNCTIONS\n",
        "# ------------------------------\n",
        "\n",
        "def summarize_pdf(chunks, gemini_chat, max_pages=5):\n",
        "    \"\"\"\n",
        "    Summarize the PDF by giving Gemini a chunked context.\n",
        "    You can limit pages using `max_pages` to control size.\n",
        "    \"\"\"\n",
        "    summaries = []\n",
        "    for i, chunk in enumerate(chunks[:max_pages]):\n",
        "        context = chunk[\"text\"]\n",
        "        prompt = f\"Summarize the following text from page {chunk['metadata']['page_no']}:\\n\\n{context}\"\n",
        "        response = gemini_chat.ask(prompt)\n",
        "        summaries.append(f\"Page {chunk['metadata']['page_no']} Summary:\\n{response}\\n\")\n",
        "    return \"\\n\".join(summaries)\n",
        "\n",
        "\n",
        "def generate_mcqs(chunks, gemini_chat, num_mcqs=5):\n",
        "    \"\"\"\n",
        "    Generate MCQs based on the PDF chunks.\n",
        "    \"\"\"\n",
        "    # Take first 10 chunks for efficiency\n",
        "    context = \"\\n\".join([chunk[\"text\"] for chunk in chunks[:10]])\n",
        "    prompt = f\"\"\"\n",
        "    Based on the following study material, generate {num_mcqs} multiple-choice questions (MCQs).\n",
        "    Each MCQ should have 4 options (A, B, C, D) and specify the correct answer clearly.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "    \"\"\"\n",
        "    response = gemini_chat.ask(prompt)\n",
        "    return response\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# RUN PIPELINE\n",
        "# ------------------------------\n",
        "\n",
        "# Example user query\n",
        "user_query = \"What are CRUD Operations?\"\n",
        "docs = retriever.get_relevant_documents(user_query)\n",
        "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "# Get answer from Gemini\n",
        "final_answer = gemini_chat.ask(f\"Given the following context, answer this: {user_query}\\n\\n{context}\")\n",
        "\n",
        "# Generate PDF summary\n",
        "pdf_summary = summarize_pdf(chunks, gemini_chat)\n",
        "\n",
        "# Generate MCQs\n",
        "mcqs = generate_mcqs(chunks, gemini_chat, num_mcqs=5)\n",
        "\n",
        "# ------------------------------\n",
        "# CREATE JSON OUTPUT\n",
        "# ------------------------------\n",
        "result_json = create_json_output(pdf_summary, user_query, final_answer, mcqs)\n",
        "\n",
        "# Print JSON\n",
        "print(result_json)\n",
        "\n",
        "# Save and download JSON\n",
        "save_json_to_file(result_json)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "84CUqYo9sXKI",
        "outputId": "ea93f616-57a8-4b73-db33-2a497027e271"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"summariser\": \"Page 1 Summary:\\nThis syllabus covers database fundamentals, including conceptual modeling (ER, EER, UML), relational database design and SQL (including normalization up to BCNF), transaction management (ACID properties, concurrency control), and an introduction to NoSQL databases (MongoDB and HBase).\\n\\n\\nPage 2 Summary:\\nUnit V introduces NoSQL databases, focusing on the CAP theorem and the data models and CRUD operations of MongoDB (document-based) and HBase (column-based).\\n\\n\\nPage 3 Summary:\\nThe text fragment \\\"Column-Family Analytical\\\" likely refers to a characteristic of column-oriented databases, but without more context, a complete summary is not possible.\\n\\n\\nPage 4 Summary:\\nNoSQL databases offer flexible data models, optimized for handling large datasets with low latency.  Document databases and wide-column databases are examples of these NoSQL types.\\n\\n\\nPage 5 Summary:\\nA distributed database system stores and processes data across multiple computers. NoSQL databases are cited as an example.\\n\\n\",\n",
            "    \"user_query\": {\n",
            "        \"query\": \"What are CRUD Operations?\",\n",
            "        \"answer\": \"CRUD operations are the four basic functions of persistent storage:\\n\\n* **Create:**  Adding new data.\\n* **Read:** Retrieving existing data.\\n* **Update:** Modifying existing data.\\n* **Delete:** Removing existing data.\\n\\nThe provided text emphasizes their importance in NoSQL databases like MongoDB and HBase.\\n\"\n",
            "    },\n",
            "    \"mcqs\": \"Here are 5 multiple-choice questions based on the provided text:\\n\\n1. **Which of the following is NOT a component of Conceptual Data Modeling as described in the syllabus?**\\n    A) Entity-Relationship model\\n    B) Enhanced-ER model\\n    C) UML class diagrams\\n    D) SQL Data Manipulation\\n\\n    **Correct Answer: D**\\n\\n2. **In relational database design, what is the purpose of normalization (up to BCNF)?**\\n    A) To increase data redundancy.\\n    B) To reduce data redundancy and improve data integrity.\\n    C) To speed up data retrieval regardless of data structure.\\n    D) To simplify the database schema without considering data integrity.\\n\\n    **Correct Answer: B**\\n\\n3.  **The ACID properties (Atomicity, Consistency, Isolation, Durability) are primarily associated with:**\\n    A) NoSQL databases\\n    B) Database normalization\\n    C) Transaction management\\n    D) Data modeling techniques\\n\\n    **Correct Answer: C**\\n\\n4. **According to the CAP theorem, which combination of properties is generally impossible to achieve simultaneously in a distributed database system?**\\n    A) Consistency, Availability, Partition Tolerance\\n    B) Consistency, Availability\\n    C) Availability, Partition Tolerance\\n    D) Consistency, Partition Tolerance\\n\\n    **Correct Answer: A** (Note:  The CAP theorem states you can only fully satisfy two of the three.)\\n\\n5. **Which type of NoSQL database is characterized by storing data as sets of key-value pairs within documents?**\\n    A) Wide-Column Database\\n    B) Graph Database\\n    C) Key-Value Store  (While technically true, the description more directly points to Document DBs)\\n    D) Document Database\\n\\n    **Correct Answer: D**\\n\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_62a78dd1-6d23-484f-b7f0-bc92135ce8ab\", \"results.json\", 3180)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}